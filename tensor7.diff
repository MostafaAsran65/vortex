diff --git a/kernel/include/vx_tensor.h b/kernel/include/vx_tensor.h
index f29c45c8..be959252 100644
--- a/kernel/include/vx_tensor.h
+++ b/kernel/include/vx_tensor.h
@@ -14,105 +14,338 @@
 #pragma once
 
 #include <stdint.h>
-#include <vx_intrinsics.h>
 #include <type_traits>
-#include <hfloats.h>
-#include <wmma_cfg.h>
-
-#ifndef NUM_THREADS
-#define NUM_THREADS 4
-#endif
+#include <cassert>
 
 namespace vortex {
 namespace tensor {
 
-template <typename It, // input type (A,B)
-          typename Ot> // output type (C,D)
-struct config {
+struct fp32 {
+  using dtype = uint32_t;
+  static constexpr uint32_t bits = 32;
+  static constexpr uint32_t id = 0;
+};
+
+struct fp16 {
+  using dtype = uint16_t;
+  static constexpr uint32_t bits = 16;
+  static constexpr uint32_t id = 1;
+};
+struct bf16 {
+  using dtype = uint16_t;
+  static constexpr uint32_t bits = 16;
+  static constexpr uint32_t id = 2;
+};
+
+struct int32 {
+  using dtype = uint32_t;
+  static constexpr uint32_t bits = 32;
+  static constexpr uint32_t id = 3;
+};
+
+struct int16 {
+  using dtype = uint16_t;
+  static constexpr uint32_t bits = 16;
+  static constexpr uint32_t id = 4;
+};
+
+struct int8 {
+  using dtype = uint8_t;
+  static constexpr uint32_t bits = 8;
+  static constexpr uint32_t id = 5;
+};
+
+enum mem_layout {
+  row_major,
+  col_major
+};
+
+template <uint32_t NT,    // number of threads per warp
+          typename It,    // input (A,B) type size in bytes
+          typename Ot,    // output (C,D) type size in bytes
+          uint32_t NR = 8,// registers per fragment
+          typename Rt = float,// register type
+          uint32_t DP = 0 // Dot-Product Length (0 for auto)
+          >
+struct config_t {
 private:
-  using cfg = wmma_config_t<NUM_THREADS, 8, sizeof(void*), It, Ot>;
+  static constexpr uint32_t clog2(uint32_t x) {
+    return (x < 2) ? 0 : (1 + clog2(x / 2));
+  }
+  static constexpr uint32_t tile_cap = NT * NR;
+  static constexpr uint32_t lg_tile_cap = clog2(tile_cap);
+  static constexpr uint32_t tile_en = lg_tile_cap / 2;
+  static constexpr uint32_t tile_em = lg_tile_cap - tile_en;
+
+  static constexpr uint32_t block_cap = NT;
+  static constexpr uint32_t lg_block_cap = clog2(block_cap);
+  static constexpr uint32_t block_en = lg_block_cap / 2;
+  static constexpr uint32_t block_em = lg_block_cap - block_en;
+
+  static constexpr uint32_t RB = sizeof(Rt);
+  static constexpr uint32_t IB = sizeof(typename It::dtype);
+  static constexpr uint32_t OB = sizeof(typename Ot::dtype);
+
 public:
-  static constexpr uint32_t tileM = cfg::tileM;
-  static constexpr uint32_t tileN = cfg::tileN;
-  static constexpr uint32_t tileK = cfg::tileK;
-};
+  static constexpr uint32_t i_ratio = RB / IB;
+  static constexpr uint32_t o_ratio = RB / OB;
+  static_assert(i_ratio * IB == RB, "RB must be multiple of IB");
+  static_assert(o_ratio * OB == RB, "RB must be multiple of OB");
+
+  static constexpr uint32_t xtileM = 1u << tile_em;
+  static constexpr uint32_t xtileN = 1u << tile_en;
+  static constexpr uint32_t xtileK = tile_cap / ((xtileM > xtileN) ? xtileM : xtileN);
+
+  static constexpr uint32_t tcM = 1u << block_em;
+  static constexpr uint32_t tcN = 1u << block_en;
+  static constexpr uint32_t tcK = (DP != 0) ? DP : (block_cap / ((tcM > tcN) ? tcM : tcN));
+
+  static constexpr uint32_t m_steps = xtileM / tcM;  // number of M steps per register
+  static constexpr uint32_t n_steps = xtileN / tcN;  // number of N steps per register
+  static constexpr uint32_t k_steps = xtileK / tcK;  // number of K steps per register
+
+  static constexpr uint32_t a_block_size = tcM * tcK;                 // size of A micro-tile
+  static constexpr uint32_t a_sub_blocks = block_cap / a_block_size;  // number of A micro-tiles per register
+  static constexpr uint32_t a_sub_steps  = m_steps / a_sub_blocks;    // number of A sub-steps per register
+
+  static constexpr uint32_t b_block_size = tcK * tcN;                 // size of B micro-tile
+  static constexpr uint32_t b_sub_blocks = block_cap / b_block_size;  // number of B micro-tiles per register
+  static constexpr uint32_t b_sub_steps  = n_steps / b_sub_blocks;    // number of B sub-steps per register
+
+  static constexpr uint32_t NRA = (xtileM * xtileK) / NT; // Number of A registers
+  static constexpr uint32_t NRB = (xtileN * xtileK) / NT; // Number of B registers
+  static constexpr uint32_t NRC = (xtileM * xtileN) / NT; // Number of C registers
+
+  static constexpr uint32_t tileM = xtileM;
+  static constexpr uint32_t tileN = xtileN;
+  static constexpr uint32_t tileK = xtileK * i_ratio; // Adjusted for input type size
+
+  using vreg_t   = Rt;
+  using input_t  = It;
+  using output_t = Ot;
 
-enum frag_use_t { matrix_d, matrix_a, matrix_b, matrix_c };
-enum layout_t { row_major, col_major };
+  static_assert(a_sub_steps != 0, "tcK is too small for tile A");
+  static_assert(b_sub_steps != 0, "tcK is too small for tile B");
 
-template <frag_use_t U, typename T, layout_t L>
-struct fragment {
-  typedef T Type;
-  static const frag_use_t Use = U;
-  static const layout_t Layout = L;
-  mf32x8_t data;
+  static_assert((xtileM * xtileK <= tile_cap), "xtileM * xtileK <= tile_cap");
+  static_assert((xtileN * xtileK <= tile_cap), "xtileN * xtileK <= tile_cap");
+  static_assert((xtileM * xtileN <= tile_cap), "xtileM * xtileN <= tile_cap");
+
+  static_assert((tcM * tcK <= block_cap), "tcM * tcK <= block_cap");
+  static_assert((tcN * tcK <= block_cap), "tcN * tcK <= block_cap");
+  static_assert((tcM * tcN <= block_cap), "tcM * tcN <= block_cap");
+
+  static_assert((xtileM % tcM) == 0, "M,m divisibility");
+  static_assert((xtileN % tcN) == 0, "N,n divisibility");
+  static_assert((xtileK % tcK) == 0, "K,k divisibility");
 };
 
-template <typename Frag>
-__attribute__((always_inline)) void fill_fragment(Frag &dst, size_t value) {
-  if constexpr (Frag::Use == matrix_d) {
-    dst.data = vx_wsetm_d_f32(value);
-  } else if constexpr (Frag::Use == matrix_a) {
-    dst.data = vx_wsetm_a_f32(value);
-  } else if constexpr (Frag::Use == matrix_b) {
-    dst.data = vx_wsetm_b_f32(value);
-  } else if constexpr (Frag::Use == matrix_c) {
-    dst.data = vx_wsetm_c_f32(value);
+#ifdef VORTEX
+
+#include <vx_intrinsics.h>
+
+namespace detail {
+  template <uint32_t begin, uint32_t end, typename Func, typename... Args>
+  inline void unrolled_for(Func&& func, Args&&... args)  {
+    if constexpr (begin < end) {
+      func(std::integral_constant<uint32_t, begin>{}, std::forward<Args>(args)...);
+      unrolled_for<begin+1, end>(std::forward<Func>(func), std::forward<Args>(args)...);
+    }
   }
-}
 
-template <typename Frag, layout_t mem_layout = row_major>
-__attribute__((always_inline)) void load_matrix_sync(Frag &dst, const void *src, size_t ldm) {
-  if constexpr (Frag::Use == matrix_a) {
-    if constexpr (Frag::Layout == mem_layout) {
-      dst.data = vx_wldm_ad_f32(src, ldm);
-    } else {
-      dst.data = vx_wldm_at_f32(src, ldm);
+  template <typename T>
+  struct raw_unsigned {
+    using type = std::conditional_t<(sizeof(T) == 1), uint8_t,
+      std::conditional_t<(sizeof(T) == 2), uint16_t,
+        std::conditional_t<(sizeof(T) == 4), uint32_t,
+          uint64_t>>>;
+  };
+  template <typename T>
+  using raw_unsigned_t = typename raw_unsigned<T>::type;
+
+  template <typename D, typename S>
+  inline __attribute__((always_inline)) D bit_fill(S src) {
+    static_assert(sizeof(D) % sizeof(S) == 0, "D must be a multiple of S in size");
+    constexpr uint32_t count = sizeof(D) / sizeof(S);
+    constexpr uint32_t bits = 8 * sizeof(S);
+    using US = raw_unsigned_t<S>;
+    using UD = raw_unsigned_t<D>;
+    auto src_u = *reinterpret_cast<const US*>(src); // bit cast
+    auto src_d = static_cast<UD>(src_u); // zero-extend
+    UD result_u(0);
+    for (uint32_t i = 0; i < count; i++) {
+      result_u |= (src_d << (i * bits));
     }
-  } else if constexpr (Frag::Use == matrix_b) {
-    if constexpr (Frag::Layout == mem_layout) {
-      dst.data = vx_wldm_bd_f32(src, ldm);
-    } else {
-      dst.data = vx_wldm_bt_f32(src, ldm);
+    return *reinterpret_cast<const D*>(&result_u);
+  }
+
+  template <typename D, typename S>
+  inline __attribute__((always_inline)) D pack_row(const S *base, uint32_t ldm) {
+    static_assert(sizeof(D) % sizeof(S) == 0, "D must be a multiple of S in size");
+    constexpr uint32_t count = sizeof(D) / sizeof(S);
+    constexpr uint32_t bits = 8 * sizeof(S);
+    using US = raw_unsigned_t<S>;
+    using UD = raw_unsigned_t<D>;
+    UD result_u(0);
+    for (uint32_t i = 0; i < count; ++i) {
+      auto src_u = *reinterpret_cast<const US*>(base); // bit cast
+      auto src_d = static_cast<UD>(src_u); // zero-extend
+      result_u |= (src_d << (i * bits));
+      base += ldm; // move to the next row
     }
-  } else {
-    static_assert(false, "Only matrix_a and matrix_b are supported!");
+    return *reinterpret_cast<const D*>(&result_u);
   }
 }
 
-template <typename Frag, layout_t mem_layout = row_major>
-__attribute__((always_inline)) void store_matrix_sync(void *dst, const Frag &src, size_t ldm) {
-  static_assert(Frag::Layout == mem_layout, "fragment layout should match memory!");
-  if constexpr (Frag::Use == matrix_c) {
-    vx_wstm_f32(dst, src.data, ldm);
-  } else if constexpr (Frag::Use == matrix_d) {
-    vx_wstm_f32(dst, src.data, ldm);
-  } else {
-    static_assert(false, "Only matrix_c or matrix_c are supported!");
+template <uint32_t NT, // number of threads per warp
+          typename It, // input type (A,B)
+          typename Ot> // output type (C,D)
+struct mma_context {
+private:
+  using cfg = config_t<NT, It, Ot>;
+  typedef float Vdata __attribute__((vector_size(8*4)));
+
+  enum frag_use_t { matrix_a, matrix_b, accumulator };
+
+  template <frag_use_t U, typename T>
+  struct fragment_t {
+    using Type = T;
+    static constexpr frag_use_t Use = U;
+    Vdata data;
+  };
+
+public:
+
+  using vreg_t   = typename cfg::vreg_t;
+  using input_t  = typename cfg::input_t::dtype;
+  using output_t = typename cfg::output_t::dtype;
+
+  static constexpr uint32_t tileM = cfg::tileM;
+  static constexpr uint32_t tileN = cfg::tileN;
+  static constexpr uint32_t tileK = cfg::tileK;
+
+  using fragment_a   = fragment_t<matrix_a, input_t>;
+  using fragment_b   = fragment_t<matrix_b, input_t>;
+  using fragment_acc = fragment_t<accumulator, output_t>;
+
+  template <typename Frag, typename T>
+  static __attribute__((always_inline)) void fill_fragment(Frag &dst, T value) {
+    auto fill_data = detail::bit_fill<vreg_t>(value);
+    auto * const pdst = reinterpret_cast<vreg_t*>(&dst.data);
+    detail::unrolled_for<0, 8>([&](auto r) {
+      pdst[r] = *reinterpret_cast<const vreg_t*>(&fill_data);
+    });
   }
-}
 
-template <typename FragD, typename FragA, typename FragB, typename FragC>
-__attribute__((always_inline)) void mma_sync(FragD &D, const FragA &A, const FragB &B, const FragC &C) {
-  static_assert(FragA::Use == matrix_a, "A must be matrix_a");
-  static_assert(FragB::Use == matrix_b, "B must be matrix_b");
-  static_assert(FragC::Use == matrix_c, "C must be matrix_c");
-  static_assert(FragD::Use == matrix_d || FragD::Use == matrix_c, "D must be matrix_d or matrix_c");
-  static_assert(std::is_same_v<typename FragA::Type, typename FragB::Type>, "A and B must have the same type");
-  static_assert(std::is_same_v<typename FragC::Type, typename FragD::Type>, "C and D must have the same type");
-
-  if constexpr (std::is_same_v<typename FragC::Type, float>
-             && std::is_same_v<typename FragA::Type, float>) {
-    if constexpr (FragD::Use == matrix_d) {
-      D.data = vx_hmma_844_d_f16_f32(A.data, B.data, C.data);
+  template <typename Frag, mem_layout src_layout = row_major>
+  static __attribute__((always_inline)) void load_matrix_sync(Frag &dst, const void *src, size_t ldm) {
+    uint32_t lane = vx_thread_id();
+    if constexpr (Frag::Use == matrix_a) {
+      // Load row-major matrix a
+      uint32_t block_idx = lane / cfg::a_block_size;
+      uint32_t lane_in_block = lane % cfg::a_block_size;
+      uint32_t elem_row = lane_in_block / cfg::tcK;
+      uint32_t elem_col = lane_in_block % cfg::tcK;
+      auto * const pdst = reinterpret_cast<vreg_t*>(&dst.data);
+      detail::unrolled_for<0, 8>([&](auto r) {
+        uint32_t block_m = (r / cfg::k_steps) * cfg::a_sub_blocks + block_idx;
+        uint32_t block_k = r % cfg::k_steps;
+        uint32_t row = block_m * cfg::tcM + elem_row;
+        uint32_t col = block_k * cfg::tcK + elem_col;
+        if constexpr (src_layout == row_major) {
+          auto base = reinterpret_cast<const input_t*>(src) + row * ldm + col * cfg::i_ratio;
+          assert(reinterpret_cast<uintptr_t>(base) % alignof(vreg_t) == 0 && "Base pointer must be aligned to 4 bytes");
+          pdst[r] = *reinterpret_cast<const vreg_t *>(base);
+        } else {
+          assert(false && "Column-major storage not yet implemented!");
+        }
+      });
+    } else if constexpr (Frag::Use == matrix_b) {
+      // Load column-major matrix b
+      uint32_t block_idx = lane / cfg::b_block_size;
+      uint32_t lane_in_block = lane % cfg::b_block_size;
+      uint32_t elem_col = lane_in_block / cfg::tcK;
+      uint32_t elem_row = lane_in_block % cfg::tcK;
+      auto * const pdst = reinterpret_cast<vreg_t*>(&dst.data);
+      detail::unrolled_for<0, 8>([&](auto r) {
+        uint32_t block_k = r / cfg::b_sub_steps;
+        uint32_t block_n = (r % cfg::b_sub_steps) * cfg::b_sub_blocks + block_idx;
+        uint32_t row = block_k * cfg::tcK + elem_row;
+        uint32_t col = block_n * cfg::tcN + elem_col;
+        if constexpr (src_layout == row_major) {
+          auto base = reinterpret_cast<const input_t*>(src) + row * ldm  * cfg::i_ratio + col;
+          if constexpr (sizeof(vreg_t) == sizeof(input_t)) {
+            pdst[r] = *reinterpret_cast<const vreg_t *>(base);
+          } else {
+            pdst[r] = detail::pack_row<vreg_t>(base, ldm);
+          }
+        } else {
+          assert(false && "Column-major storage not yet implemented!");
+        }
+      });
     } else {
-      D.data = vx_hmma_844_c_f16_f32(A.data, B.data, C.data);
+      // Load accumulator matrix (C)
+      uint32_t elem_row = lane / cfg::tcN;
+      uint32_t elem_col = lane % cfg::tcN;
+      auto * const pdst = reinterpret_cast<vreg_t*>(&dst.data);
+      detail::unrolled_for<0, 8>([&](auto r) {
+        uint32_t block_m = r / cfg::n_steps;
+        uint32_t block_n = r % cfg::n_steps;
+        uint32_t row = block_m * cfg::tcM + elem_row;
+        uint32_t col = block_n * cfg::tcN + elem_col;
+        if constexpr (src_layout == row_major) {
+          auto base = reinterpret_cast<const output_t*>(src) + row * ldm + col;
+          if constexpr (sizeof(vreg_t) == sizeof(output_t)) {
+            pdst[r] = *reinterpret_cast<const vreg_t *>(base);
+          } else {
+            vreg_t tmp(0);
+            *reinterpret_cast<output_t*>(&tmp) = *base;
+            pdst[r] = tmp;
+          }
+        } else {
+          assert(false && "Column-major storage not yet implemented!");
+        }
+      });
     }
-  } else {
-    static_assert(false, "Unsupported type!");
   }
-}
 
-} // namespace wmma
+  template <typename Frag, mem_layout dst_layout = row_major>
+  static __attribute__((always_inline)) void store_matrix_sync(void *dst, const Frag &src, size_t ldm) {
+    static_assert(Frag::Use == accumulator, "Only accumulator fragment can be stored");
+    uint32_t lane = vx_thread_id();
+    uint32_t elem_row = lane / cfg::tcN;
+    uint32_t elem_col = lane % cfg::tcN;
+    auto * const psrc = reinterpret_cast<const vreg_t*>(&src.data);
+    detail::unrolled_for<0, 8>([&](auto r) {
+      uint32_t block_m = r / cfg::n_steps;
+      uint32_t block_n = r % cfg::n_steps;
+      uint32_t row = block_m * cfg::tcM + elem_row;
+      uint32_t col = block_n * cfg::tcN + elem_col;
+      if constexpr (dst_layout == row_major) {
+        auto base = reinterpret_cast<output_t*>(dst) + row * ldm + col;
+        if constexpr (sizeof(vreg_t) == sizeof(output_t)) {
+          *reinterpret_cast<vreg_t*>(base) = psrc[r];
+        } else {
+          vreg_t tmp(psrc[r]);
+          *base = *reinterpret_cast<const output_t*>(&tmp);
+        }
+      } else {
+        assert(false && "Column-major storage not yet implemented!");
+      }
+    });
+  }
+
+  template <typename FragD, typename FragA, typename FragB, typename FragC>
+  static __attribute__((always_inline)) void mma_sync(FragD &D, const FragA &A, const FragB &B, const FragC &C) {
+    static_assert(FragA::Use == matrix_a, "A must be matrix_a");
+    static_assert(FragB::Use == matrix_b, "B must be matrix_b");
+    static_assert(FragC::Use == accumulator, "C must be accumulator");
+    static_assert(FragD::Use == accumulator, "D must be accumulator");
+    //--
+  }
+};
+
+#endif // VORTEX
 
-} // namespace vortex
\ No newline at end of file
+} // namespace vxwmma
+} // namespace vortex
diff --git a/sim/common/wmma_cfg.h b/sim/common/wmma_cfg.h
deleted file mode 100644
index 1397c6a8..00000000
--- a/sim/common/wmma_cfg.h
+++ /dev/null
@@ -1,78 +0,0 @@
-#include <algorithm>
-
-namespace vortex {
-
-template <uint32_t NT,    // number of threads per warp
-          uint32_t NR,    // registers per fragment
-          uint32_t XB,    // vector element type size in bytes
-          typename It,    // input type (A,B)
-          typename Ot,    // output type (C,D)
-          uint32_t DP = 0 // Dot-Product Length (0 for auto)
-          >
-struct wmma_config_t {
-private:
-  static constexpr uint32_t clog2(uint32_t x) {
-    return (x < 2) ? 0 : (1 + clog2(x / 2));
-  }
-
-  static constexpr uint32_t XlenB = XB;
-  static constexpr uint32_t OlenB = sizeof(Ot);
-  static constexpr uint32_t IlenB = sizeof(It);
-
-  static constexpr uint32_t tile_cap = NT * NR;
-  static constexpr uint32_t lg_tile_cap = clog2(tile_cap);
-  static constexpr uint32_t tile_en = lg_tile_cap / 2;
-  static constexpr uint32_t tile_em = lg_tile_cap - tile_en;
-
-  static constexpr uint32_t block_cap = NT;
-  static constexpr uint32_t lg_block_cap = clog2(block_cap);
-  static constexpr uint32_t block_en = lg_block_cap / 2;
-  static constexpr uint32_t block_em = lg_block_cap - block_en;
-
-public:
-  static constexpr uint32_t NumThreads = NT;
-  static constexpr uint32_t NumRegs = NR;
-
-  static constexpr uint32_t tileM = 1u << tile_em;
-  static constexpr uint32_t tileN = 1u << tile_en;
-  static constexpr uint32_t tileK = tile_cap / ((tileM > tileN) ? tileM : tileN);
-
-  static constexpr uint32_t tcM = 1u << block_em;
-  static constexpr uint32_t tcN = 1u << block_en;
-  static constexpr uint32_t tcK = (DP != 0) ? DP : (block_cap / ((tcM > tcN) ? tcM : tcN));
-
-  static constexpr uint32_t m_steps = tileM / tcM;
-  static constexpr uint32_t n_steps = tileN / tcN;
-  static constexpr uint32_t k_steps = tileK / tcK;
-
-  static constexpr uint32_t a_block_size = tcM * tcK;             // size of A micro-tile
-  static constexpr uint32_t a_sub_blocks = NT / a_block_size;     // number of A micro-tiles per register
-  static constexpr uint32_t a_sub_steps  = m_steps / a_sub_blocks;// number of A sub-steps per register
-
-  static constexpr uint32_t b_block_size = tcK * tcN;             // size of B micro-tile
-  static constexpr uint32_t b_sub_blocks = NT / b_block_size;     // number of B micro-tiles per register
-  static constexpr uint32_t b_sub_steps  = n_steps / b_sub_blocks;// number of B sub-steps per register
-
-  static_assert(a_sub_steps != 0, "tcK is too small for tile A");
-  static_assert(b_sub_steps != 0, "tcK is too small for tile B");
-
-  static_assert((tileM * tileK <= NR * NT), "tileM*tileK <= NR * NT");
-  static_assert((tileN * tileK <= NR * NT), "tileN*tileK <= NR * NT");
-  static_assert((tileM * tileN <= NR * NT), "tileM*tileN <= NR * NT");
-
-  static_assert((tcM * tcK <= NT), "tcM*tcK <= NT");
-  static_assert((tcN * tcK <= NT), "tcN*tcK <= NT");
-  static_assert((tcM * tcN <= NT), "tcM*tcN <= NT");
-
-  static_assert((tileM % tcM) == 0, "M,m divisibility");
-  static_assert((tileN % tcN) == 0, "N,n divisibility");
-  static_assert((tileK % tcK) == 0, "K,k divisibility");
-
-  static_assert(XB % sizeof(It) == 0, "XB must be multiple of sizeof(It)");
-  static_assert(XB % sizeof(Ot) == 0, "XB must be multiple of sizeof(Ot)");
-
-  using input_t  = It;
-  using output_t = Ot;
-};
-
-}
\ No newline at end of file
diff --git a/tests/kernel/common.mk b/tests/kernel/common.mk
index a2369e9e..605fe725 100644
--- a/tests/kernel/common.mk
+++ b/tests/kernel/common.mk
@@ -27,7 +27,7 @@ CP  = $(RISCV_TOOLCHAIN_PATH)/bin/$(RISCV_PREFIX)-objcopy
 
 CFLAGS += -O3 -mcmodel=medany -fno-exceptions -nostartfiles -nostdlib -fdata-sections -ffunction-sections
 CFLAGS += -I$(VORTEX_HOME)/kernel/include -I$(ROOT_DIR)/hw -I$(VORTEX_HOME)/sim/common
-CFLAGS += -DXLEN_$(XLEN) -DNDEBUG
+CFLAGS += -DVORTEX -DXLEN_$(XLEN) -DNDEBUG
 
 LIBC_LIB += -L$(LIBC_VORTEX)/lib -lm -lc
 LIBC_LIB += $(LIBCRT_VORTEX)/lib/baremetal/libclang_rt.builtins-riscv$(XLEN).a
diff --git a/tests/opencl/common.mk b/tests/opencl/common.mk
index f8e0ee75..f1a0db36 100644
--- a/tests/opencl/common.mk
+++ b/tests/opencl/common.mk
@@ -37,7 +37,7 @@ VX_LIBS += $(LIBCRT_VORTEX)/lib/baremetal/libclang_rt.builtins-riscv$(XLEN).a
 
 VX_CFLAGS  += -O3 -mcmodel=medany --sysroot=$(RISCV_SYSROOT) --gcc-toolchain=$(RISCV_TOOLCHAIN_PATH)
 VX_CFLAGS  += -fno-rtti -fno-exceptions -nostartfiles -nostdlib -fdata-sections -ffunction-sections
-VX_CFLAGS  += -I$(ROOT_DIR)/hw -I$(VORTEX_HOME)/kernel/include -DXLEN_$(XLEN) -DNDEBUG
+VX_CFLAGS  += -I$(ROOT_DIR)/hw -I$(VORTEX_HOME)/kernel/include -DVORTEX -DXLEN_$(XLEN) -DNDEBUG
 VX_CFLAGS  += -Xclang -target-feature -Xclang +vortex
 VX_CFLAGS  += -Xclang -target-feature -Xclang +zicond
 VX_CFLAGS  += -mllvm -disable-loop-idiom-all	# disable memset/memcpy loop replacement
diff --git a/tests/regression/common.mk b/tests/regression/common.mk
index 708aa2bb..caf7308a 100644
--- a/tests/regression/common.mk
+++ b/tests/regression/common.mk
@@ -48,7 +48,7 @@ VX_CP  = $(LLVM_VORTEX)/bin/llvm-objcopy
 
 VX_CFLAGS += -O3 -mcmodel=medany -fno-rtti -fno-exceptions -nostartfiles -nostdlib -fdata-sections -ffunction-sections
 VX_CFLAGS += -I$(VORTEX_HOME)/kernel/include -I$(ROOT_DIR)/hw -I$(VORTEX_HOME)/sim/common
-VX_CFLAGS += -DXLEN_$(XLEN)
+VX_CFLAGS += -DVORTEX -DXLEN_$(XLEN)
 VX_CFLAGS += -DNDEBUG
 
 VX_LIBS += -L$(LIBC_VORTEX)/lib -lm -lc
@@ -59,7 +59,7 @@ VX_LIBS += $(LIBCRT_VORTEX)/lib/baremetal/libclang_rt.builtins-riscv$(XLEN).a
 VX_LDFLAGS += -Wl,-Bstatic,--gc-sections,-T,$(VORTEX_HOME)/kernel/scripts/link$(XLEN).ld,--defsym=STARTUP_ADDR=$(STARTUP_ADDR) $(VORTEX_KN_PATH)/libvortex.a $(VX_LIBS)
 
 CXXFLAGS += -std=c++17 -Wall -Wextra -pedantic -Wfatal-errors
-CXXFLAGS += -I$(VORTEX_HOME)/runtime/include -I$(ROOT_DIR)/hw -I$(VORTEX_HOME)/sim/common
+CXXFLAGS += -I$(VORTEX_HOME)/runtime/include -I$(VORTEX_HOME)/kernel/include -I$(ROOT_DIR)/hw -I$(VORTEX_HOME)/sim/common
 
 LDFLAGS += -L$(VORTEX_RT_PATH) -lvortex
 
diff --git a/tests/regression/sgemm_tpu/Makefile b/tests/regression/sgemm_tpu/Makefile
index 28ad15cf..61367235 100644
--- a/tests/regression/sgemm_tpu/Makefile
+++ b/tests/regression/sgemm_tpu/Makefile
@@ -9,6 +9,8 @@ SRCS := $(SRC_DIR)/main.cpp
 
 VX_SRCS := $(SRC_DIR)/kernel.cpp
 
+CXXFLAGS += -DSRC_PATH='"$(SRC_DIR)"'
+
 OPTS ?= -n32
 
 include ../common.mk
\ No newline at end of file
diff --git a/tests/regression/sgemm_tpu/common.h b/tests/regression/sgemm_tpu/common.h
index 957b4168..e154e57f 100644
--- a/tests/regression/sgemm_tpu/common.h
+++ b/tests/regression/sgemm_tpu/common.h
@@ -2,19 +2,7 @@
 #define _COMMON_H_
 
 #include <stdint.h>
-#include <hfloats.h>
-
-#ifndef NUM_THREADS
-#define NUM_THREADS 4
-#endif
-
-#ifndef I_TYPE
-#define I_TYPE float
-#endif
-
-#ifndef O_TYPE
-#define O_TYPE float
-#endif
+#include <vx_tensor.h>
 
 typedef struct {
   uint32_t grid_dim[2];
diff --git a/tests/regression/sgemm_tpu/kernel.cpp b/tests/regression/sgemm_tpu/kernel.cpp
index a278e3d3..723ec3dd 100644
--- a/tests/regression/sgemm_tpu/kernel.cpp
+++ b/tests/regression/sgemm_tpu/kernel.cpp
@@ -2,46 +2,60 @@
 #include <vx_spawn.h>
 #include <vx_tensor.h>
 
-using cfg = tensor::config<I_TYPE, O_TYPE>;
+#ifndef NUM_THREADS
+#define NUM_THREADS 4
+#endif
+
+#ifndef I_TYPE
+#define I_TYPE vortex::tensor::fp32
+#endif
+
+#ifndef O_TYPE
+#define O_TYPE vortex::tensor::fp32
+#endif
+
+namespace vts = vortex::tensor;
+using ctx = vts::mma_context<NUM_THREADS, I_TYPE, O_TYPE>;
 
 void kernel_body(kernel_arg_t *__UNIFORM__ arg) {
-  auto A = reinterpret_cast<I_TYPE *>(arg->A_addr);
-  auto B = reinterpret_cast<I_TYPE *>(arg->B_addr);
-  auto C = reinterpret_cast<O_TYPE *>(arg->C_addr);
+  auto pA = reinterpret_cast<ctx::input_t *>(arg->A_addr);
+  auto pB = reinterpret_cast<ctx::input_t *>(arg->B_addr);
+  auto pC = reinterpret_cast<ctx::output_t *>(arg->C_addr);
 
-  tensor::fragment<tensor::matrix_a, I_TYPE, tensor::row_major> fragA;
-  tensor::fragment<tensor::matrix_b, I_TYPE, tensor::col_major> fragB;
-  tensor::fragment<tensor::matrix_c, O_TYPE, tensor::row_major> fragC;
+  ctx::fragment_a   fragA;
+  ctx::fragment_b   fragB;
+  ctx::fragment_acc fragC;
 
   // calculate tile row & column based on block index
-  uint32_t tile_row = blockIdx.y * cfg::tileM;
-  uint32_t tile_col = blockIdx.x * cfg::tileN;
+  uint32_t tile_row = blockIdx.y * ctx::tileM;
+  uint32_t tile_col = blockIdx.x * ctx::tileN;
 
+  uint32_t M = arg->M;
   uint32_t N = arg->N;
   uint32_t K = arg->K;
 
   // Initialize accumulator tile to zero
-  tensor::fill_fragment(fragC, 0);
+  ctx::fill_fragment(fragC, 0);
 
-  for (int i = 0; i < K; i += cfg::tileK) {
+  for (int i = 0; i < K; i += ctx::tileK) {
     // Load A tile
-    auto tileA = A + (tile_row * K + i);
-    tensor::load_matrix_sync(fragA, tileA, K);
+    auto pTileA = pA + tile_row * K + i;
+    ctx::load_matrix_sync(fragA, pTileA, K);
 
     // Load B tile
-    auto tileB = B + (i * K + tile_col);
-    tensor::load_matrix_sync(fragB, tileB, K);
+    auto pTileB = pB + i * N + tile_col;
+    ctx::load_matrix_sync(fragB, pTileB, N);
 
     // Matrix multiply-accumulate: c += a * b
-    tensor::mma_sync(fragC, fragA, fragB, fragC);
+    ctx::mma_sync(fragC, fragA, fragB, fragC);
   }
 
   // Store the computed C tile
-  auto tileC = C + (tile_row * N + tile_col);
-  tensor::store_matrix_sync(tileC, fragC, N);
+  auto pTileC = pC + tile_row * N + tile_col;
+  ctx::store_matrix_sync(pTileC, fragC, N);
 }
 
 int main() {
-  kernel_arg_t *arg = (kernel_arg_t *)csr_read(VX_CSR_MSCRATCH);
+  auto arg = (kernel_arg_t *)csr_read(VX_CSR_MSCRATCH);
   return vx_spawn_threads(2, arg->grid_dim, arg->block_dim, (vx_kernel_func_cb)kernel_body, arg);
 }
diff --git a/tests/regression/sgemm_tpu/main.cpp b/tests/regression/sgemm_tpu/main.cpp
index afb49187..dc3dc0d7 100644
--- a/tests/regression/sgemm_tpu/main.cpp
+++ b/tests/regression/sgemm_tpu/main.cpp
@@ -1,13 +1,19 @@
 #include "common.h"
 #include <chrono>
 #include <cmath>
-#include <hfloats.h>
 #include <iostream>
 #include <string.h>
 #include <unistd.h>
 #include <vector>
 #include <vortex.h>
-#include <wmma_cfg.h>
+#include "float16.h"
+#include "bfloat16.h"
+
+namespace vts = vortex::tensor;
+
+#ifndef SRC_PATH
+#define SRC_PATH "."
+#endif
 
 #define FLOAT_ULP 6
 
@@ -24,7 +30,22 @@
 ///////////////////////////////////////////////////////////////////////////////
 
 template <typename Type>
-class Comparator {};
+class Comparator {
+public:
+
+  template <typename SType>
+  static void matmul_cpu(Type *C, const SType *A, const SType *B, uint32_t M, uint32_t N, uint32_t K) {
+    for (uint32_t m = 0; m < M; ++m) {
+      for (uint32_t n = 0; n < N; ++n) {
+        Type sum(0);
+        for (uint32_t k = 0; k < K; ++k) {
+          sum += Type(A[m * K + k] * B[k * N + n]);
+        }
+        C[m * N + n] = sum;
+      }
+    }
+  }
+};
 
 template <>
 class Comparator<int8_t> {
@@ -47,11 +68,31 @@ public:
 };
 
 template <>
-class Comparator<int> {
+class Comparator<int16_t> {
 public:
   static const char *type_str() {
     return "int8";
   }
+  static int8_t generate() {
+    return (int8_t)rand();
+  }
+  static bool compare(int a, int b, int index, int errors) {
+    if (a != b) {
+      if (errors < 100) {
+        printf("*** error: [%d] expected=%d, actual=%d\n", index, b, a);
+      }
+      return false;
+    }
+    return true;
+  }
+};
+
+template <>
+class Comparator<int> {
+public:
+  static const char *type_str() {
+    return "int32";
+  }
   static int generate() {
     return (int)rand();
   }
@@ -67,13 +108,41 @@ public:
 };
 
 template <>
-class Comparator<vortex::half_t> {
+class Comparator<float16_t> {
 public:
   static const char *type_str() {
     return "f16";
   }
-  static vortex::half_t generate() {
-    return static_cast<vortex::half_t>(float(rand()) / RAND_MAX);
+  static float16_t generate() {
+    return static_cast<float16_t>(float(rand()) / RAND_MAX);
+  }
+  static bool compare(float a, float b, int index, int errors) {
+    union fi_t {
+      float f;
+      int32_t i;
+    };
+    fi_t fa, fb;
+    fa.f = a;
+    fb.f = b;
+    auto d = std::abs(fa.i - fb.i);
+    if (d > FLOAT_ULP) {
+      if (errors < 100) {
+        printf("*** error: [%d] expected=%f, actual=%f\n", index, b, a);
+      }
+      return false;
+    }
+    return true;
+  }
+};
+
+template <>
+class Comparator<bfloat16_t> {
+public:
+  static const char *type_str() {
+    return "bf16";
+  }
+  static bfloat16_t generate() {
+    return static_cast<bfloat16_t>(float(rand()) / RAND_MAX);
   }
   static bool compare(float a, float b, int index, int errors) {
     union fi_t {
@@ -122,22 +191,142 @@ public:
   }
 };
 
-static void matmul_cpu(O_TYPE *C, const I_TYPE *A, const I_TYPE *B, uint32_t M, uint32_t N, uint32_t K) {
-  for (uint32_t m = 0; m < M; ++m) {
-    for (uint32_t n = 0; n < N; ++n) {
-      O_TYPE sum(0);
-      for (uint32_t k = 0; k < K; ++k) {
-        sum += O_TYPE(A[m * K + k] * B[k * N + n]);
-      }
-      C[m * N + n] = sum;
+int buildKernel(const std::string &options) {
+  std::string cmd = "CONFIGS='" + options + "' make -C '" + SRC_PATH + "' kernel.vxbin";
+  int exitCode = std::system(cmd.c_str());
+  if (exitCode == -1) {
+    std::cerr << "Error: failed to invoke shell to run make.\n";
+  }
+  return exitCode;
+}
+
+template <uint32_t NT, typename It, typename Ot>
+void get_mma_tile_size(uint32_t *tileM, uint32_t *tileN, uint32_t *tileK) {
+  using cfg = vts::config_t<NT, It, Ot>;
+  *tileM = cfg::tileM;
+  *tileN = cfg::tileN;
+  *tileK = cfg::tileK;
+}
+
+template <typename It, typename Ot>
+void get_mma_tile_size(uint32_t NT, uint32_t *tileM, uint32_t *tileN, uint32_t *tileK) {
+   switch (NT) {
+  case 1:
+    get_mma_tile_size<1, It, Ot>(tileM, tileN, tileK);
+    break;
+  case 2:
+    get_mma_tile_size<2, It, Ot>(tileM, tileN, tileK);
+    break;
+  case 4:
+    get_mma_tile_size<4, It, Ot>(tileM, tileN, tileK);
+    break;
+  case 8:
+    get_mma_tile_size<8, It, Ot>(tileM, tileN, tileK);
+    break;
+  case 16:
+    get_mma_tile_size<16, It, Ot>(tileM, tileN, tileK);
+    break;
+  case 32:
+    get_mma_tile_size<32, It, Ot>(tileM, tileN, tileK);
+    break;
+  case 64:
+    get_mma_tile_size<64, It, Ot>(tileM, tileN, tileK);
+    break;
+  default:
+    std::cout << "Error: unsupported number of threads: " << NT << " threads!" << std::endl;
+    std::abort();
+  }
+}
+
+void get_mma_tile_size(uint32_t NT, uint32_t IT, uint32_t OT, uint32_t *tileM, uint32_t *tileN, uint32_t *tileK) {
+  switch (OT) {
+  case 0:
+    switch (IT) {
+    case 0:
+      get_mma_tile_size<vts::fp32, vts::fp32>(NT, tileM, tileN, tileK);
+      break;
+    case 1:
+      get_mma_tile_size<vts::fp16, vts::fp32>(NT, tileM, tileN, tileK);
+      break;
+    case 2:
+      get_mma_tile_size<vts::bf16, vts::fp32>(NT, tileM, tileN, tileK);
+      break;
+    default:
+      std::cout << "Error: unsupported mma type: " << IT << " -> " << OT << "!" << std::endl;
+      std::abort();
+    }
+    break;
+  case 1:
+    switch (IT) {
+    case 1:
+      get_mma_tile_size<vts::fp16, vts::fp16>(NT, tileM, tileN, tileK);
+      break;
+    default:
+      std::cout << "Error: unsupported mma type: " << IT << " -> " << OT << "!" << std::endl;
+      std::abort();
+    }
+    break;
+  case 2:
+    switch (IT) {
+    case 2:
+      get_mma_tile_size<vts::bf16, vts::bf16>(NT, tileM, tileN, tileK);
+      break;
+    default:
+      std::cout << "Error: unsupported mma type: " << IT << " -> " << OT << "!" << std::endl;
+      std::abort();
+    }
+    break;
+  case 3:
+    switch (IT) {
+    case 3:
+      get_mma_tile_size<vts::int32, vts::int32>(NT, tileM, tileN, tileK);
+      break;
+    case 4:
+      get_mma_tile_size<vts::int16, vts::int32>(NT, tileM, tileN, tileK);
+      break;
+    case 5:
+      get_mma_tile_size<vts::int8, vts::int32>(NT, tileM, tileN, tileK);
+      break;
+    default:
+      std::cout << "Error: unsupported mma type: " << IT << " -> " << OT << "!" << std::endl;
+      std::abort();
+    }
+    break;
+  case 4:
+    switch (IT) {
+    case 4:
+      get_mma_tile_size<vts::int16, vts::int16>(NT, tileM, tileN, tileK);
+      break;
+    case 5:
+      get_mma_tile_size<vts::int8, vts::int16>(NT, tileM, tileN, tileK);
+      break;
+    default:
+      std::cout << "Error: unsupported mma type: " << IT << " -> " << OT << "!" << std::endl;
+      std::abort();
     }
+    break;
+  case 5:
+    switch (IT) {
+    case 5:
+      get_mma_tile_size<vts::int8, vts::int8>(NT, tileM, tileN, tileK);
+      break;
+    default:
+      std::cout << "Error: unsupported mma type: " << IT << " -> " << OT << "!" << std::endl;
+      std::abort();
+    }
+    break;
+  default:
+    std::cout << "Error: unsupported output type: " << OT << "!" << std::endl;
+    std::abort();
   }
 }
 
 const char *kernel_file = "kernel.vxbin";
-uint32_t M = 32;
-uint32_t N = 32;
-uint32_t K = 32;
+uint32_t M = 32;  // matrix A rows
+uint32_t N = 32;  // matrix B columns
+uint32_t K = 32;  // matrix A columns / matrix B rows
+uint32_t I = 1;   // input type
+uint32_t O = 0;   // output type
 
 vx_device_h device = nullptr;
 vx_buffer_h A_buffer = nullptr;
@@ -147,14 +336,23 @@ vx_buffer_h krnl_buffer = nullptr;
 vx_buffer_h args_buffer = nullptr;
 kernel_arg_t kernel_arg = {};
 
+std::string last_build_options;
+
 static void show_usage() {
   std::cout << "Vortex Test." << std::endl;
-  std::cout << "Usage: [-m: m] [-n N] [-k: K] [-h: help]" << std::endl;
+  std::cout << "Usage: [-m: m] [-n N] [-k: K] [-i: input type] [-o: output type] [-h: help]" << std::endl;
+  std::cout << "Matrix types:" << std::endl;
+  std::cout << "  0: float32" << std::endl;
+  std::cout << "  1: float16" << std::endl;
+  std::cout << "  2: bfloat16" << std::endl;
+  std::cout << "  3: int32" << std::endl;
+  std::cout << "  4: int16" << std::endl;
+  std::cout << "  5: int8" << std::endl;
 }
 
 static void parse_args(int argc, char **argv) {
   int c;
-  while ((c = getopt(argc, argv, "m:n:k:h")) != -1) {
+  while ((c = getopt(argc, argv, "m:n:k:i:o:h")) != -1) {
     switch (c) {
     case 'm':
       M = atoi(optarg);
@@ -165,6 +363,12 @@ static void parse_args(int argc, char **argv) {
     case 'k':
       K = atoi(optarg);
       break;
+    case 'i':
+      I = atoi(optarg);
+      break;
+    case 'o':
+      O = atoi(optarg);
+      break;
     case 'h':
       show_usage();
       exit(0);
@@ -205,31 +409,8 @@ int main(int argc, char *argv[]) {
   }
   std::cout << "GPU warp size: " << NT << " threads" << std::endl;
 
-  uint64_t isa_flags;
-  RT_CHECK(vx_dev_caps(device, VX_CAPS_ISA_FLAGS, &isa_flags));
-  uint32_t XlenB = VX_ISA_ARCH(isa_flags) / 8;
-  std::cout << "GPU XLEN: " << 8 * XlenB << std::endl;
-
   uint32_t tileM, tileN, tileK;
-
-  vortex::wmma_cfg_t<NUM_THREADS, 8, uint32_t, O_TYPE, I_TYPE> cfg32;
-  vortex::wmma_cfg_t<NUM_THREADS, 8, uint64_t, O_TYPE, I_TYPE> cfg64;
-
-  if (XlenB == 4) {
-    std::cout << "Using 32-bit configuration." << std::endl;
-    tileM = cfg32.tileM;
-    tileN = cfg32.tileN;
-    tileK = cfg32.tileK;
-  } else if (XlenB == 8) {
-    std::cout << "Using 64-bit configuration." << std::endl;
-    tileM = cfg64.tileM;
-    tileN = cfg64.tileN;
-    tileK = cfg64.tileK;
-  } else {
-    std::cout << "Error: unsupported XLEN: " << XlenB << " bytes!" << std::endl;
-    return -1;
-  }
-
+  get_mma_tile_size(NT, I, O, &tileM, &tileN, &tileK);
   std::cout << "GPU tensor tileM=" << tileM << ", tileN=" << tileM << ", tileK=" << tileK << std::endl;
 
   if ((M % tileM) != 0) {
@@ -257,6 +438,10 @@ int main(int argc, char *argv[]) {
   std::cout << "matrix B: " << K << "x" << N << std::endl;
   std::cout << "matrix C: " << M << "x" << N << std::endl;
 
+  // build the kernel
+  std::cout << "build the kernel" << std::endl;
+  std::string build_options = "-DTM=" + std::to_string(M) + " N=" + std::to_string(N) + " K=" + std::to_string(K) +" I=" + std::to_string(I) + " O=" + std::to_string(O);
+
   // set block size to warp size
   kernel_arg.grid_dim[0] = N / tileN;
   kernel_arg.grid_dim[1] = M / tileM;
diff --git a/tests/regression/sgemm_tpu/tensor_generic.cpp b/tests/regression/sgemm_tpu/tensor_generic.cpp
index 6b567d0a..1d43a749 100644
--- a/tests/regression/sgemm_tpu/tensor_generic.cpp
+++ b/tests/regression/sgemm_tpu/tensor_generic.cpp
@@ -17,7 +17,7 @@ using float32_t = float;
 #endif
 
 #ifndef OTYPE
-#define OTYPE float32_t
+#define OTYPE float16_t
 #endif
 
 #ifndef ITYPE
@@ -411,7 +411,7 @@ private:
     for (uint32_t z = 0; z < tcK * i_ratio; ++z) {
       auto a_val = static_cast<Ot>(a[z]);
       auto b_val = static_cast<Ot>(b[z]);
-      acc += a_val * b_val;
+      acc = a_val * b_val + acc;
     }
     Xt ret(0);
     *reinterpret_cast<Ot*>(&ret) = acc;
@@ -528,7 +528,7 @@ public:
         for (uint32_t k = 0; k < tileK; ++k) {
           auto a = static_cast<Ot>(fragA_(row, k));
           auto b = static_cast<Ot>(fragB_(k, col));
-          sum += a * b;
+          sum = a * b + sum;;
         }
         fragRef_(row, col) = sum + fragC_(row, col);
       }
diff --git a/third_party/ramulator b/third_party/ramulator
--- a/third_party/ramulator
+++ b/third_party/ramulator
@@ -1 +1 @@
-Subproject commit e62c84a6f0e06566ba6e182d308434b4532068a5
+Subproject commit e62c84a6f0e06566ba6e182d308434b4532068a5-dirty
